# SpatialV2A: Visual-Guided High-fidelity Spatial Audio Generation

[![Paper](https://img.shields.io/badge/Paper-arXiv%3A2601.15017-brightgreen.svg)](https://arxiv.org/pdf/2601.15017)
[![Dataset](https://img.shields.io/badge/Dataset-Coming%20Soon-blue.svg)]()
[![Demo](https://img.shields.io/badge/Demo-Available-orange.svg)]()
[![Webpage](https://img.shields.io/badge/Webpage-Coming%20Soon-purple.svg)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Authors**: Yanan Wang, Linjie Ren, Zihao Li, Junyi Wang, Tian Gan (School of Computer Science and Technology, Shandong University)

---

## üìù Abstract
While video-to-audio generation has achieved remarkable progress in semantic and temporal alignment, most existing studies focus solely on these aspects, paying limited attention to the spatial perception and immersive quality of the synthesized audio. This limitation stems largely from current models‚Äô reliance on mono audio datasets, which lack the binaural spatial information needed to learn visual-to-spatial audio mappings.

To address this gap, we introduce two key contributions: we construct BinauralVGGSound, the first large-scale video-binaural audio dataset designed to support spatially aware video-to-audio generation; and we propose an end-to-end spatial audio generation framework guided by visual cues, which explicitly models spatial features. Our framework incorporates a visual-guided audio spatialization module that ensures the generated audio exhibits realistic spatial attributes and layered spatial depth while maintaining semantic and temporal alignment.

Experiments show that our approach substantially outperforms state-of-the-art models in spatial fidelity and delivers a more immersive auditory experience, without sacrificing temporal or semantic consistency. All datasets, code, and model checkpoints will be publicly released to facilitate future research.

---

## üîß Core Pipelines
### 1. Dataset Construction Pipeline
![Dataset Construction Pipeline](assert/dataset_construction_pipeline.png)
*Figure: Pipeline for constructing the large-scale BinauralVGGSound dataset*

### 2. SpatialV2A Framework Pipeline
![SpatialV2A Framework Pipeline](assert/SpatialV2A_pipeline.png)
*Figure: End-to-end visual-guided spatial audio generation pipeline of SpatialV2A*

---

## üé¨ Demo Videos
Below are representative video demos (the audio in the videos is the high-fidelity spatial audio generated by our model; **wearing headphones is strongly recommended** to experience the spatial effect):

- **Description**: chainsawing tree  
  [Click to play demo video](assert/1MgcrYdYas0_000030.mp4)

- **Description**: waves crashing on a beach  
  [Click to play demo video](assert/8sd513xQzV4_000030.mp4)

- **Description**: rope skipping  
  [Click to play demo video](assert/9ge5GwYO-pU_000016.mp4)

> Note: All demo videos are from the test set of BinauralVGGSound dataset.

---

## üìã Todo List
- [ ] Release full source code of SpatialV2A framework
- [ ] Open access to BinauralVGGSound dataset (187k video-binaural audio pairs)
- [ ] Release pre-trained model checkpoints
- [ ] Update official webpage with detailed technical documentation
- [ ] Add more diverse demo videos and audio samples
- [ ] Provide inference code and user guide for quick deployment

---

## üìù Citation
If you find our work useful for your research, please cite the following BibTeX:
```bibtex
@misc{wang2026spatialv2avisualguidedhighfidelityspatial,
      title={SpatialV2A: Visual-Guided High-fidelity Spatial Audio Generation}, 
      author={Yanan Wang and Linjie Ren and Zihao Li and Junyi Wang and Tian Gan},
      year={2026},
      eprint={2601.15017},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2601.15017}, 
}
